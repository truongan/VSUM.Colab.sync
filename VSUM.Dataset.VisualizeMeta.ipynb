{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/truongan/VSUM.Colab.sync/blob/main/VSUM.Dataset.VisualizeMeta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc6EvPPhhHwq"
      },
      "source": [
        "# Model Loading - Visualization\n",
        "\n",
        "* Extract features from an arbitrary intermediate layer: https://keras.io/api/applications/#usage-examples-for-image-classification-models\n",
        "*  Keras models: https://www.tensorflow.org/api_docs/python/tf/keras/applications/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaK7rqiEmnjs"
      },
      "source": [
        "# Kiểm tra xem số chiều của từng layer\n",
        "import tensorflow\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "# model = VGG16()\n",
        "# #print(model.summary())\n",
        "\n",
        "\n",
        "# model = ResNet50()\n",
        "# #print(model.summary())\n",
        "\n",
        "# model = InceptionV3()\n",
        "model = tensorflow.keras.applications.InceptionResNetV2()\n",
        "print(model.summary())\n",
        "\n",
        "# model = EfficientNetB7()\n",
        "# print(model.summary())\n",
        "\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqYDt0p4qRXC"
      },
      "source": [
        "# Extract Features - Last Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXa0iRhbs7dh"
      },
      "source": [
        "# Thử nghiệm rút trích feature của last layer\n",
        "# Lưu ý input size của từng pre-trained network là khác nhau\n",
        "# Ảnh sẽ được lưu từ URL trên Internet\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# model = VGG16(weights='imagenet', include_top=False)\n",
        "# target_size=(224, 224)\n",
        "\n",
        "# model = ResNet50(weights='imagenet', include_top=False)\n",
        "# target_size=(224, 224)\n",
        "\n",
        "model = InceptionV3(weights='imagenet', include_top=False, pooling = 'avg')\n",
        "target_size=(299, 299)\n",
        "\n",
        "\n",
        "img_path = 'elephant.jpg'\n",
        "import urllib.request\n",
        "imgURL = \"https://i.pinimg.com/originals/e1/d0/ec/e1d0ec7162f7405c6cb53a90e92ab641.jpg\"\n",
        "\n",
        "urllib.request.urlretrieve(imgURL, img_path)\n",
        "\n",
        "img = image.load_img(img_path, target_size)\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# trick khi view\n",
        "print(x.shape)\n",
        "plt.imshow(x/255.)\n",
        "\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "features = model.predict(x)\n",
        "\n",
        "print(features.shape)\n",
        "print(decode_predictions(features))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdAOxqhwXcGA"
      },
      "source": [
        "# Video Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWzWHd1KShLj"
      },
      "source": [
        "# https://www.youtube.com/watch?v=bsNWQLzANpI\n",
        "#https://stackoverflow.com/questions/40713268/download-youtube-video-using-python-to-a-certain-directory\n",
        "\n",
        "!pip install pytube\n",
        "\n",
        "from pytube import YouTube\n",
        "import os\n",
        "\n",
        "def downloadYouTube(videourl, path):\n",
        "\n",
        "  yt = YouTube(videourl)\n",
        "  yt = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "  yt.download(path)\n",
        "\n",
        "szVideoID = 'xypzmu5mMPY'\n",
        "szVideoURL = 'https://www.youtube.com/watch?v=' + szVideoID\n",
        "szVideoFile = 'M-tp-mrmsc'\n",
        "\n",
        "\n",
        "downloadYouTube(szVideoURL, szVideoFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVYV_AlJTOOW"
      },
      "source": [
        "# Hiển thị YouTube video\n",
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo(szVideoID)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjiRbzxfYXxh"
      },
      "source": [
        "# VSUM\n",
        "\n",
        "An bắt đầu re-use code từ chỗ này. Copy xuống nhé!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtUhuSh0ODpX"
      },
      "source": [
        "# https://www.pyimagesearch.com/2019/07/15/video-classification-with-keras-and-deep-learning/\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "#model = VGG16(weights='imagenet', include_top=False)\n",
        "#target_size=(224, 224)\n",
        "\n",
        "#model = ResNet50(weights='imagenet', include_top=False)\n",
        "#target_size=(224, 224)\n",
        "\n",
        "model = InceptionV3(weights='imagenet', include_top=False)\n",
        "target_size=(299, 299)\n",
        "\n",
        "# initialize the video stream, pointer to output video file, and\n",
        "# frame dimensions\n",
        "\n",
        "szVideoFilePath =  '/content/M-tp-mrmsc/SƠN TÙNG M-TP  MUỘN RỒI MÀ SAO CÒN  OFFICIAL MUSIC VIDEO.mp4'\n",
        "vs = cv2.VideoCapture(szVideoFilePath)\n",
        "writer = None\n",
        "(W, H) = (None, None)\n",
        "# loop over frames from the video file stream\n",
        "\n",
        "count = 0\n",
        "while True:\n",
        "\t# read the next frame from the file\n",
        "  (grabbed, frame) = vs.read()\n",
        "\t# if the frame was not grabbed, then we have reached the end\n",
        "\t# of the stream\n",
        "  if not grabbed:\n",
        "    break\n",
        "\t# if the frame dimensions are empty, grab them\n",
        "  if W is None or H is None:\n",
        "    (H, W) = frame.shape[:2]\n",
        "  # clone the output frame, then convert it from BGR to RGB\n",
        "\t# ordering, resize the frame to a fixed 224x224, and then\n",
        "\t# perform mean subtraction\n",
        "  output = frame.copy()\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  frame = cv2.resize(frame, target_size).astype(\"float32\")\n",
        "  x = image.img_to_array(frame)\n",
        "\n",
        "  if (count == 500):\n",
        "    plt.imshow(x/255.)\n",
        "    break\n",
        "\n",
        "  if (count % 100 == 0):\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    features = model.predict(x)\n",
        "    print(count)\n",
        "\n",
        "  #print(features.shape)\n",
        "  count = count + 1\n",
        "  #break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYx2r4-kQN_t"
      },
      "source": [
        "# Extract feature and store in H5 file\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17otP_oNpa5x"
      },
      "source": [
        "## Explorer h5 file structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPm5JM3n-Fov"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMaVZR29yJbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca7cf508-2896-460e-d165-d5aa83fa4770"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AX4XfWgtDzwHb2Nb88SvKWXMSpjPbymaSMCXYeScbC2pDV2oKKse58wiTXU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMz4Q-3P-FIE"
      },
      "source": [
        "## Data\n",
        "Structured h5 files with the video features and annotations of the SumMe and TVSum datasets are available within the \"data\" folder. The GoogleNet features of the video frames were extracted by [Ke Zhang](https://github.com/kezhang-cs) and [Wei-Lun Chao](https://github.com/pujols) and the h5 files were obtained from [Kaiyang Zhou](https://github.com/KaiyangZhou/pytorch-vsumm-reinforce). These files have the following structure:\n",
        "<pre>\n",
        "/key\n",
        "    /features                 2D-array with shape (n_steps, feature-dimension)\n",
        "    /gtscore                  1D-array with shape (n_steps), stores ground truth improtance score (used for training, e.g. regression loss)\n",
        "    /user_summary             2D-array with shape (num_users, n_frames), each row is a binary vector (used for test)\n",
        "    /change_points            2D-array with shape (num_segments, 2), each row stores indices of a segment\n",
        "    /n_frame_per_seg          1D-array with shape (num_segments), indicates number of frames in each segment\n",
        "    /n_frames                 number of frames in original video\n",
        "    /picks                    positions of subsampled frames in original video\n",
        "    /n_steps                  number of subsampled frames\n",
        "    /gtsummary                1D-array with shape (n_steps), ground truth summary provided by user (used for training, e.g. maximum likelihood)\n",
        "    /video_name (optional)    original video name, only available for SumMe dataset\n",
        "</pre>\n",
        "Original videos and annotations for each dataset are also available in the authors' project webpages:\n",
        "- TVSum dataset: https://github.com/yalesong/tvsum\n",
        "- SumMe dataset: https://gyglim.github.io/me/vsum/index.html#benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRFfck68palQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7361fd15-5ced-419a-dccb-5e7a3bcda318"
      },
      "source": [
        "h5file = '/content/drive/MyDrive/VSum/Colab/VASNet/datasets/eccv16_dataset_tvsum_google_pool5.h5' #AN\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "data = h5py.File(h5file, 'r')\n",
        "\n",
        "\n",
        "keys = [i for i in data.keys()]\n",
        "\n",
        "first = data[keys[1]]\n",
        "print(keys)\n",
        "keys_for_one_video = [k for k in first.keys()]\n",
        "print(keys_for_one_video)\n",
        "\n",
        "\n",
        "# print(\"List of video and their number of frames\")\n",
        "# for i in data:\n",
        "#   print(i, np.asarray(data[i]['n_frames']))\n",
        "\n",
        "for i in keys_for_one_video:\n",
        "# for i in ['gtscore', 'user_summary']:\n",
        "  a = np.asarray(first[i])\n",
        "  print(first[i])\n",
        "  print(a)\n",
        "\n",
        "for i in ['gtscore', 'user_summary']:\n",
        "  a = np.asarray(first[i])\n",
        "  print(first[i])\n",
        "  print(a)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['video_1', 'video_10', 'video_11', 'video_12', 'video_13', 'video_14', 'video_15', 'video_16', 'video_17', 'video_18', 'video_19', 'video_2', 'video_20', 'video_21', 'video_22', 'video_23', 'video_24', 'video_25', 'video_26', 'video_27', 'video_28', 'video_29', 'video_3', 'video_30', 'video_31', 'video_32', 'video_33', 'video_34', 'video_35', 'video_36', 'video_37', 'video_38', 'video_39', 'video_4', 'video_40', 'video_41', 'video_42', 'video_43', 'video_44', 'video_45', 'video_46', 'video_47', 'video_48', 'video_49', 'video_5', 'video_50', 'video_6', 'video_7', 'video_8', 'video_9']\n",
            "['change_points', 'features', 'gtscore', 'gtsummary', 'n_frame_per_seg', 'n_frames', 'n_steps', 'picks', 'user_summary']\n",
            "<HDF5 dataset \"change_points\": shape (27, 2), type \"<i8\">\n",
            "[[   0  164]\n",
            " [ 165  793]\n",
            " [ 794  883]\n",
            " [ 884  962]\n",
            " [ 963 1038]\n",
            " [1039 1125]\n",
            " [1126 1295]\n",
            " [1296 1393]\n",
            " [1394 1928]\n",
            " [1929 2009]\n",
            " [2010 2116]\n",
            " [2117 2189]\n",
            " [2190 2257]\n",
            " [2258 2305]\n",
            " [2306 2400]\n",
            " [2401 2478]\n",
            " [2479 2555]\n",
            " [2556 2634]\n",
            " [2635 3108]\n",
            " [3109 3185]\n",
            " [3186 3365]\n",
            " [3366 3452]\n",
            " [3453 3538]\n",
            " [3539 3633]\n",
            " [3634 3723]\n",
            " [3724 3909]\n",
            " [3910 3994]]\n",
            "<HDF5 dataset \"features\": shape (267, 1024), type \"<f4\">\n",
            "[[0.00000000e+00 2.90918499e-02 8.18118220e-04 ... 0.00000000e+00\n",
            "  4.72546965e-02 0.00000000e+00]\n",
            " [8.29735189e-04 1.03136385e-02 2.12522931e-02 ... 5.98567612e-02\n",
            "  6.64655715e-02 7.63073273e-04]\n",
            " [0.00000000e+00 7.96074420e-03 3.39382305e-03 ... 3.61849703e-02\n",
            "  2.91168038e-03 6.20457344e-03]\n",
            " ...\n",
            " [3.54691409e-03 1.84713602e-02 3.05140913e-02 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [2.74300529e-03 3.78390290e-02 8.43391847e-03 ... 0.00000000e+00\n",
            "  9.45492793e-05 0.00000000e+00]\n",
            " [1.08302711e-03 2.25363951e-02 2.25291923e-02 ... 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "<HDF5 dataset \"gtscore\": shape (267,), type \"<f4\">\n",
            "[0.0875 0.0875 0.0875 0.0875 0.0625 0.0625 0.0625 0.0625 0.05   0.05\n",
            " 0.05   0.05   0.2375 0.2375 0.2375 0.2375 0.1875 0.1875 0.1875 0.1875\n",
            " 0.1375 0.1375 0.1375 0.1375 0.175  0.175  0.175  0.175  0.125  0.125\n",
            " 0.125  0.125  0.15   0.15   0.15   0.15   0.1625 0.1625 0.1625 0.1625\n",
            " 0.175  0.175  0.175  0.175  0.15   0.15   0.15   0.15   0.1625 0.1625\n",
            " 0.1625 0.1625 0.3375 0.3375 0.3375 0.3375 0.3375 0.3375 0.3375 0.3375\n",
            " 0.2125 0.2125 0.2125 0.2125 0.425  0.425  0.425  0.425  0.425  0.425\n",
            " 0.425  0.425  0.4125 0.4125 0.4125 0.4125 0.425  0.425  0.425  0.425\n",
            " 0.2375 0.2375 0.2375 0.2375 0.275  0.275  0.275  0.275  0.3875 0.3875\n",
            " 0.3875 0.3875 0.225  0.225  0.225  0.225  0.2625 0.2625 0.2625 0.2625\n",
            " 0.25   0.25   0.25   0.25   0.275  0.275  0.275  0.275  0.3    0.3\n",
            " 0.3    0.3    0.2875 0.2875 0.2875 0.2875 0.3375 0.3375 0.3375 0.3375\n",
            " 0.2875 0.2875 0.2875 0.2875 0.3125 0.3125 0.3125 0.3125 0.225  0.225\n",
            " 0.225  0.225  0.4125 0.4125 0.4125 0.4125 0.35   0.35   0.35   0.35\n",
            " 0.3125 0.3125 0.3125 0.3125 0.325  0.325  0.325  0.325  0.2625 0.2625\n",
            " 0.2625 0.2625 0.3625 0.3625 0.3625 0.3625 0.2875 0.2875 0.2875 0.2875\n",
            " 0.3125 0.3125 0.3125 0.3125 0.175  0.175  0.175  0.175  0.15   0.15\n",
            " 0.15   0.15   0.3    0.3    0.3    0.3    0.125  0.125  0.125  0.125\n",
            " 0.125  0.125  0.125  0.125  0.1375 0.1375 0.1375 0.1375 0.175  0.175\n",
            " 0.175  0.175  0.1375 0.1375 0.1375 0.1375 0.1625 0.1625 0.1625 0.1625\n",
            " 0.1375 0.1375 0.1375 0.1375 0.125  0.125  0.125  0.125  0.1125 0.1125\n",
            " 0.1125 0.1125 0.3125 0.3125 0.3125 0.3125 0.2375 0.2375 0.2375 0.2375\n",
            " 0.4    0.4    0.4    0.4    0.175  0.175  0.175  0.175  0.2625 0.2625\n",
            " 0.2625 0.2625 0.25   0.25   0.25   0.25   0.125  0.125  0.125  0.125\n",
            " 0.1625 0.1625 0.1625 0.1625 0.5125 0.5125 0.5125 0.5125 0.375  0.375\n",
            " 0.375  0.375  0.35   0.35   0.35   0.35   0.525  0.525  0.525  0.525\n",
            " 0.325  0.325  0.325  0.325  0.325  0.325  0.325 ]\n",
            "<HDF5 dataset \"gtsummary\": shape (267,), type \"<f4\">\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0.]\n",
            "<HDF5 dataset \"n_frame_per_seg\": shape (27,), type \"<i8\">\n",
            "[165 629  90  79  76  87 170  98 535  81 107  73  68  48  95  78  77  79\n",
            " 474  77 180  87  86  95  90 186  85]\n",
            "<HDF5 dataset \"n_frames\": shape (), type \"<i8\">\n",
            "3995\n",
            "<HDF5 dataset \"n_steps\": shape (), type \"<i8\">\n",
            "267\n",
            "<HDF5 dataset \"picks\": shape (267,), type \"<i8\">\n",
            "[   0   15   30   45   60   75   90  105  120  135  150  165  180  195\n",
            "  210  225  240  255  270  285  300  315  330  345  360  375  390  405\n",
            "  420  435  450  465  480  495  510  525  540  555  570  585  600  615\n",
            "  630  645  660  675  690  705  720  735  750  765  780  795  810  825\n",
            "  840  855  870  885  900  915  930  945  960  975  990 1005 1020 1035\n",
            " 1050 1065 1080 1095 1110 1125 1140 1155 1170 1185 1200 1215 1230 1245\n",
            " 1260 1275 1290 1305 1320 1335 1350 1365 1380 1395 1410 1425 1440 1455\n",
            " 1470 1485 1500 1515 1530 1545 1560 1575 1590 1605 1620 1635 1650 1665\n",
            " 1680 1695 1710 1725 1740 1755 1770 1785 1800 1815 1830 1845 1860 1875\n",
            " 1890 1905 1920 1935 1950 1965 1980 1995 2010 2025 2040 2055 2070 2085\n",
            " 2100 2115 2130 2145 2160 2175 2190 2205 2220 2235 2250 2265 2280 2295\n",
            " 2310 2325 2340 2355 2370 2385 2400 2415 2430 2445 2460 2475 2490 2505\n",
            " 2520 2535 2550 2565 2580 2595 2610 2625 2640 2655 2670 2685 2700 2715\n",
            " 2730 2745 2760 2775 2790 2805 2820 2835 2850 2865 2880 2895 2910 2925\n",
            " 2940 2955 2970 2985 3000 3015 3030 3045 3060 3075 3090 3105 3120 3135\n",
            " 3150 3165 3180 3195 3210 3225 3240 3255 3270 3285 3300 3315 3330 3345\n",
            " 3360 3375 3390 3405 3420 3435 3450 3465 3480 3495 3510 3525 3540 3555\n",
            " 3570 3585 3600 3615 3630 3645 3660 3675 3690 3705 3720 3735 3750 3765\n",
            " 3780 3795 3810 3825 3840 3855 3870 3885 3900 3915 3930 3945 3960 3975\n",
            " 3990]\n",
            "<HDF5 dataset \"user_summary\": shape (20, 3995), type \"<f4\">\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 1. 1.]]\n",
            "<HDF5 dataset \"gtscore\": shape (267,), type \"<f4\">\n",
            "[0.0875 0.0875 0.0875 0.0875 0.0625 0.0625 0.0625 0.0625 0.05   0.05\n",
            " 0.05   0.05   0.2375 0.2375 0.2375 0.2375 0.1875 0.1875 0.1875 0.1875\n",
            " 0.1375 0.1375 0.1375 0.1375 0.175  0.175  0.175  0.175  0.125  0.125\n",
            " 0.125  0.125  0.15   0.15   0.15   0.15   0.1625 0.1625 0.1625 0.1625\n",
            " 0.175  0.175  0.175  0.175  0.15   0.15   0.15   0.15   0.1625 0.1625\n",
            " 0.1625 0.1625 0.3375 0.3375 0.3375 0.3375 0.3375 0.3375 0.3375 0.3375\n",
            " 0.2125 0.2125 0.2125 0.2125 0.425  0.425  0.425  0.425  0.425  0.425\n",
            " 0.425  0.425  0.4125 0.4125 0.4125 0.4125 0.425  0.425  0.425  0.425\n",
            " 0.2375 0.2375 0.2375 0.2375 0.275  0.275  0.275  0.275  0.3875 0.3875\n",
            " 0.3875 0.3875 0.225  0.225  0.225  0.225  0.2625 0.2625 0.2625 0.2625\n",
            " 0.25   0.25   0.25   0.25   0.275  0.275  0.275  0.275  0.3    0.3\n",
            " 0.3    0.3    0.2875 0.2875 0.2875 0.2875 0.3375 0.3375 0.3375 0.3375\n",
            " 0.2875 0.2875 0.2875 0.2875 0.3125 0.3125 0.3125 0.3125 0.225  0.225\n",
            " 0.225  0.225  0.4125 0.4125 0.4125 0.4125 0.35   0.35   0.35   0.35\n",
            " 0.3125 0.3125 0.3125 0.3125 0.325  0.325  0.325  0.325  0.2625 0.2625\n",
            " 0.2625 0.2625 0.3625 0.3625 0.3625 0.3625 0.2875 0.2875 0.2875 0.2875\n",
            " 0.3125 0.3125 0.3125 0.3125 0.175  0.175  0.175  0.175  0.15   0.15\n",
            " 0.15   0.15   0.3    0.3    0.3    0.3    0.125  0.125  0.125  0.125\n",
            " 0.125  0.125  0.125  0.125  0.1375 0.1375 0.1375 0.1375 0.175  0.175\n",
            " 0.175  0.175  0.1375 0.1375 0.1375 0.1375 0.1625 0.1625 0.1625 0.1625\n",
            " 0.1375 0.1375 0.1375 0.1375 0.125  0.125  0.125  0.125  0.1125 0.1125\n",
            " 0.1125 0.1125 0.3125 0.3125 0.3125 0.3125 0.2375 0.2375 0.2375 0.2375\n",
            " 0.4    0.4    0.4    0.4    0.175  0.175  0.175  0.175  0.2625 0.2625\n",
            " 0.2625 0.2625 0.25   0.25   0.25   0.25   0.125  0.125  0.125  0.125\n",
            " 0.1625 0.1625 0.1625 0.1625 0.5125 0.5125 0.5125 0.5125 0.375  0.375\n",
            " 0.375  0.375  0.35   0.35   0.35   0.35   0.525  0.525  0.525  0.525\n",
            " 0.325  0.325  0.325  0.325  0.325  0.325  0.325 ]\n",
            "<HDF5 dataset \"user_summary\": shape (20, 3995), type \"<f4\">\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 1. 1.]\n",
            " [0. 0. 0. ... 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "Tq2_n4UgAskB",
        "outputId": "63460da4-022b-4f73-9c07-79e2a382e675"
      },
      "source": [
        "print(min(np.asarray(first['gtscore'])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ba163ccca1fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gtscore'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "G0VxkmPRX5Ix",
        "outputId": "e406d689-050b-4bc5-9376-49c420609bcf"
      },
      "source": [
        "h5_to_file_name['video_10']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'akI8YFjEmUw.mp4'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxcPq80ezu9j"
      },
      "source": [
        "n_frames = np.asarray(first['n_frames'])\n",
        "n_steps = np.asarray(first['n_steps'])\n",
        "picks = np.asarray(first['picks'])\n",
        "\n",
        "print(picks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snmk4JHA_6XV"
      },
      "source": [
        "### It seem that 1 out of 15th frames is sampled\n",
        "\n",
        "### Nonne of the 50 video has the same number of frames, yay!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyukMlQWvK6t"
      },
      "source": [
        "## Build a dictionary to lookup video name from n_frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-LBPPguvIc4"
      },
      "source": [
        "h5file = '/content/drive/MyDrive/VSum/Colab/VASNet/datasets/eccv16_dataset_tvsum_google_pool5.h5' #AN\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "data = h5py.File(h5file, 'r')\n",
        "\n",
        "\n",
        "keys = [i for i in data.keys()]\n",
        "\n",
        "d = dict()\n",
        "\n",
        "for i in data:\n",
        "  d[ np.asarray(data[i]['n_frames']) + 0 ] = i\n",
        "  # print ( np.asarray(data[i]['n_frames']) + 0 )\n",
        "#print(d)\n",
        "\n",
        "n_frames_to_video_name_in_h5 = {10597: 'video_1', 3995: 'video_10', 4700: 'video_11', 13511: 'video_12', 3532: 'video_13', 4853: 'video_14', 4324: 'video_15', 9534: 'video_16', 5846: 'video_17', 9731: 'video_18', 5742: 'video_19', 4688: 'video_2', 6241: 'video_20', 19406: 'video_21', 5661: 'video_22', 5631: 'video_23', 4356: 'video_24', 6580: 'video_25', 3312: 'video_26', 10917: 'video_27', 8281: 'video_28', 17527: 'video_29', 14019: 'video_3', 4005: 'video_30', 5412: 'video_31', 3802: 'video_32', 13365: 'video_33', 3705: 'video_34', 4463: 'video_35', 7959: 'video_36', 4009: 'video_37', 2941: 'video_38', 4165: 'video_39', 7210: 'video_4', 11414: 'video_40', 8073: 'video_41', 5939: 'video_42', 4931: 'video_43', 4304: 'video_44', 2500: 'video_45', 15307: 'video_46', 4740: 'video_47', 3896: 'video_48', 5971: 'video_49', 3327: 'video_5', 6912: 'video_50', 9671: 'video_6', 4468: 'video_7', 9870: 'video_8', 7010: 'video_9'}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mS7qPqYZ8VV"
      },
      "source": [
        "## Trying to load TVSum dataset to extract feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgKPTRQiqiEd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "753At082Z6JE"
      },
      "source": [
        "video_dir = '/content/drive/MyDrive/VSum/Colab/ydata-tvsum50-v1_1/video' ##AN\n",
        "video_list = [\"0tmA_C6XwfM.mp4\", \"EE-bNr36nyA.mp4\", \"JgHubY5Vw3Y.mp4\", \"VuWGsYPqAX8.mp4\", \"37rzWOQsNIw.mp4\", \"eQu1rNs0an0.mp4\", \"JKpqYvAdIsw.mp4\", \"WG0MBPpPC6I.mp4\", \"3eYKfiOEJNs.mp4\", \"-esJrBWj2d8.mp4\", \"kLxoNp-UchI.mp4\", \"WxtbjNsCQ8A.mp4\", \"4wU_LUjG5Ic.mp4\", \"EYqVtI9YWJA.mp4\", \"LRw_obCPUt0.mp4\", \"XkqCExn6_Us.mp4\", \"91IHQYk1IQM.mp4\", \"fWutDQy1nnY.mp4\", \"NyBmCxDoHJU.mp4\", \"xmEERLqJ2kU.mp4\", \"98MoyGZKHXc.mp4\", \"GsAD1KT1xo8.mp4\", \"oDXZc0tZe04.mp4\", \"_xMr-HKMfVA.mp4\", \"akI8YFjEmUw.mp4\", \"gzDbaEs1Rlg.mp4\", \"PJrm840pAUI.mp4\", \"xwqBXPGE9pQ.mp4\", \"AwmHb44_ouw.mp4\", \"Hl-__g2gn_A.mp4\", \"qqR6AEXwxoQ.mp4\", \"xxdtq8mxegs.mp4\", \"b626MiF1ew4.mp4\", \"HT5vyqe0Xaw.mp4\", \"RBCABdttQmI.mp4\", \"XzYM3PfTM4w.mp4\", \"Bhxk-O1Y7Ho.mp4\", \"i3wAGJaaktw.mp4\", \"Se3oxnaPsz0.mp4\", \"Yi4Ij2NM7U4.mp4\", \"byxOvuiIJV0.mp4\", \"iVt07TCkFM0.mp4\", \"sTEELN-vY30.mp4\", \"z_6gVvQb2d0.mp4\", \"cjibtmSLxQ4.mp4\", \"J0nA4VgnoCo.mp4\", \"uGu_10sucQo.mp4\", \"E11zDS9XGzg.mp4\", \"jcoYJXDG9sw.mp4\", \"vdmoEJ5YbrQ.mp4\"]\n",
        "test_video = video_dir + \"/\" + video_list[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdFdA-kEw7_O"
      },
      "source": [
        "### Get n_frames_to_video_file_name\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn7VLrufw6vm"
      },
      "source": [
        "video_dir = '/content/drive/MyDrive/VSum/Colab/ydata-tvsum50-v1_1/video' ##AN\n",
        "video_list = [\"0tmA_C6XwfM.mp4\", \"EE-bNr36nyA.mp4\", \"JgHubY5Vw3Y.mp4\", \"VuWGsYPqAX8.mp4\", \"37rzWOQsNIw.mp4\", \"eQu1rNs0an0.mp4\", \"JKpqYvAdIsw.mp4\", \"WG0MBPpPC6I.mp4\", \"3eYKfiOEJNs.mp4\", \"-esJrBWj2d8.mp4\", \"kLxoNp-UchI.mp4\", \"WxtbjNsCQ8A.mp4\", \"4wU_LUjG5Ic.mp4\", \"EYqVtI9YWJA.mp4\", \"LRw_obCPUt0.mp4\", \"XkqCExn6_Us.mp4\", \"91IHQYk1IQM.mp4\", \"fWutDQy1nnY.mp4\", \"NyBmCxDoHJU.mp4\", \"xmEERLqJ2kU.mp4\", \"98MoyGZKHXc.mp4\", \"GsAD1KT1xo8.mp4\", \"oDXZc0tZe04.mp4\", \"_xMr-HKMfVA.mp4\", \"akI8YFjEmUw.mp4\", \"gzDbaEs1Rlg.mp4\", \"PJrm840pAUI.mp4\", \"xwqBXPGE9pQ.mp4\", \"AwmHb44_ouw.mp4\", \"Hl-__g2gn_A.mp4\", \"qqR6AEXwxoQ.mp4\", \"xxdtq8mxegs.mp4\", \"b626MiF1ew4.mp4\", \"HT5vyqe0Xaw.mp4\", \"RBCABdttQmI.mp4\", \"XzYM3PfTM4w.mp4\", \"Bhxk-O1Y7Ho.mp4\", \"i3wAGJaaktw.mp4\", \"Se3oxnaPsz0.mp4\", \"Yi4Ij2NM7U4.mp4\", \"byxOvuiIJV0.mp4\", \"iVt07TCkFM0.mp4\", \"sTEELN-vY30.mp4\", \"z_6gVvQb2d0.mp4\", \"cjibtmSLxQ4.mp4\", \"J0nA4VgnoCo.mp4\", \"uGu_10sucQo.mp4\", \"E11zDS9XGzg.mp4\", \"jcoYJXDG9sw.mp4\", \"vdmoEJ5YbrQ.mp4\"]\n",
        "\n",
        "\n",
        "import cv2\n",
        "\n",
        "d = dict()\n",
        "for i in video_list:\n",
        "  video_path = video_dir + \"/\" + i\n",
        "  vs = cv2.VideoCapture(video_path)\n",
        "  n_frames = 0\n",
        "  while True:\n",
        "    # read the next frame from the file\n",
        "    (grabbed, frame) = vs.read()\n",
        "    # if the frame was not grabbed, then we have reached the end\n",
        "    # of the stream\n",
        "    if not grabbed:\n",
        "      break\n",
        "    n_frames += 1\n",
        "\n",
        "  d[n_frames] = i\n",
        "  print(\"grabing frame for \" , i)\n",
        "\n",
        "print(d)\n",
        "\n",
        "n_frames_to_video_file_name = {3532: '0tmA_C6XwfM.mp4', 2941: 'EE-bNr36nyA.mp4', 4304: 'JgHubY5Vw3Y.mp4', 5412: 'VuWGsYPqAX8.mp4', 5742: '37rzWOQsNIw.mp4', 4931: 'eQu1rNs0an0.mp4', 3802: 'JKpqYvAdIsw.mp4', 9535: 'WG0MBPpPC6I.mp4', 4853: '3eYKfiOEJNs.mp4', 6912: '-esJrBWj2d8.mp4', 3896: 'kLxoNp-UchI.mp4', 7959: 'WxtbjNsCQ8A.mp4', 4005: '4wU_LUjG5Ic.mp4', 5939: 'EYqVtI9YWJA.mp4', 6241: 'LRw_obCPUt0.mp4', 5631: 'XkqCExn6_Us.mp4', 3312: '91IHQYk1IQM.mp4', 17527: 'fWutDQy1nnY.mp4', 4740: 'NyBmCxDoHJU.mp4', 13365: 'xmEERLqJ2kU.mp4', 4688: '98MoyGZKHXc.mp4', 4356: 'GsAD1KT1xo8.mp4', 11414: 'oDXZc0tZe04.mp4', 4463: '_xMr-HKMfVA.mp4', 3995: 'akI8YFjEmUw.mp4', 7210: 'gzDbaEs1Rlg.mp4', 6580: 'PJrm840pAUI.mp4', 7010: 'xwqBXPGE9pQ.mp4', 10597: 'AwmHb44_ouw.mp4', 5846: 'Hl-__g2gn_A.mp4', 8073: 'qqR6AEXwxoQ.mp4', 4324: 'xxdtq8mxegs.mp4', 5661: 'b626MiF1ew4.mp4', 9671: 'HT5vyqe0Xaw.mp4', 10917: 'RBCABdttQmI.mp4', 3327: 'XzYM3PfTM4w.mp4', 13511: 'Bhxk-O1Y7Ho.mp4', 4700: 'i3wAGJaaktw.mp4', 4166: 'Se3oxnaPsz0.mp4', 9731: 'Yi4Ij2NM7U4.mp4', 3705: 'byxOvuiIJV0.mp4', 2500: 'iVt07TCkFM0.mp4', 4468: 'sTEELN-vY30.mp4', 8281: 'z_6gVvQb2d0.mp4', 19406: 'cjibtmSLxQ4.mp4', 14019: 'J0nA4VgnoCo.mp4', 4009: 'uGu_10sucQo.mp4', 15307: 'E11zDS9XGzg.mp4', 5971: 'jcoYJXDG9sw.mp4', 9870: 'vdmoEJ5YbrQ.mp4'}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys8Gyutc0Jsq"
      },
      "source": [
        "n_frames_to_video_file_name = {3532: '0tmA_C6XwfM.mp4', 2941: 'EE-bNr36nyA.mp4', 4304: 'JgHubY5Vw3Y.mp4', 5412: 'VuWGsYPqAX8.mp4', 5742: '37rzWOQsNIw.mp4', 4931: 'eQu1rNs0an0.mp4', 3802: 'JKpqYvAdIsw.mp4', 9535: 'WG0MBPpPC6I.mp4', 4853: '3eYKfiOEJNs.mp4', 6912: '-esJrBWj2d8.mp4', 3896: 'kLxoNp-UchI.mp4', 7959: 'WxtbjNsCQ8A.mp4', 4005: '4wU_LUjG5Ic.mp4', 5939: 'EYqVtI9YWJA.mp4', 6241: 'LRw_obCPUt0.mp4', 5631: 'XkqCExn6_Us.mp4', 3312: '91IHQYk1IQM.mp4', 17527: 'fWutDQy1nnY.mp4', 4740: 'NyBmCxDoHJU.mp4', 13365: 'xmEERLqJ2kU.mp4', 4688: '98MoyGZKHXc.mp4', 4356: 'GsAD1KT1xo8.mp4', 11414: 'oDXZc0tZe04.mp4', 4463: '_xMr-HKMfVA.mp4', 3995: 'akI8YFjEmUw.mp4', 7210: 'gzDbaEs1Rlg.mp4', 6580: 'PJrm840pAUI.mp4', 7010: 'xwqBXPGE9pQ.mp4', 10597: 'AwmHb44_ouw.mp4', 5846: 'Hl-__g2gn_A.mp4', 8073: 'qqR6AEXwxoQ.mp4', 4324: 'xxdtq8mxegs.mp4', 5661: 'b626MiF1ew4.mp4', 9671: 'HT5vyqe0Xaw.mp4', 10917: 'RBCABdttQmI.mp4', 3327: 'XzYM3PfTM4w.mp4', 13511: 'Bhxk-O1Y7Ho.mp4', 4700: 'i3wAGJaaktw.mp4', 4166: 'Se3oxnaPsz0.mp4', 9731: 'Yi4Ij2NM7U4.mp4', 3705: 'byxOvuiIJV0.mp4', 2500: 'iVt07TCkFM0.mp4', 4468: 'sTEELN-vY30.mp4', 8281: 'z_6gVvQb2d0.mp4', 19406: 'cjibtmSLxQ4.mp4', 14019: 'J0nA4VgnoCo.mp4', 4009: 'uGu_10sucQo.mp4', 15307: 'E11zDS9XGzg.mp4', 5971: 'jcoYJXDG9sw.mp4', 9870: 'vdmoEJ5YbrQ.mp4'}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI8D0XYf0Le-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed356adb-cbaa-4062-b158-eb7589281a4b"
      },
      "source": [
        "file_name_to_h5 = dict()\n",
        "h5_to_file_name = dict()\n",
        "\n",
        "for i in n_frames_to_video_file_name:\n",
        "  if i not in n_frames_to_video_name_in_h5:\n",
        "    print(\"No video in h5 with n_frames \", i, n_frames_to_video_file_name[i])\n",
        "  else:\n",
        "    # print(i, n_frames_to_video_name_in_h5[i], )\n",
        "    file_name_to_h5[n_frames_to_video_file_name[i]] =  n_frames_to_video_name_in_h5[i]\n",
        "    h5_to_file_name[n_frames_to_video_name_in_h5[i]] =  n_frames_to_video_file_name[i]\n",
        "\n",
        " #  No video in h5 with n_frames  9535 WG0MBPpPC6I.mp4\n",
        " #  No video in h5 with n_frames  4166 Se3oxnaPsz0.mp4\n",
        "\n",
        "for i in n_frames_to_video_name_in_h5:\n",
        "  if i not in n_frames_to_video_file_name:\n",
        "    print(\"No video file with n_frames \", i, n_frames_to_video_name_in_h5[i])\n",
        "\n",
        "\n",
        "\n",
        "file_name_to_h5['WG0MBPpPC6I.mp4'] = 'video_16'\n",
        "h5_to_file_name['video_16'] = 'WG0MBPpPC6I.mp4'\n",
        "\n",
        "file_name_to_h5['Se3oxnaPsz0.mp4'] = 'video_39'\n",
        "h5_to_file_name['video_39'] = 'Se3oxnaPsz0.mp4'\n",
        "\n",
        "print(file_name_to_h5)\n",
        "print(h5_to_file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No video in h5 with n_frames  9535 WG0MBPpPC6I.mp4\n",
            "No video in h5 with n_frames  4166 Se3oxnaPsz0.mp4\n",
            "No video file with n_frames  9534 video_16\n",
            "No video file with n_frames  4165 video_39\n",
            "{'0tmA_C6XwfM.mp4': 'video_13', 'EE-bNr36nyA.mp4': 'video_38', 'JgHubY5Vw3Y.mp4': 'video_44', 'VuWGsYPqAX8.mp4': 'video_31', '37rzWOQsNIw.mp4': 'video_19', 'eQu1rNs0an0.mp4': 'video_43', 'JKpqYvAdIsw.mp4': 'video_32', '3eYKfiOEJNs.mp4': 'video_14', '-esJrBWj2d8.mp4': 'video_50', 'kLxoNp-UchI.mp4': 'video_48', 'WxtbjNsCQ8A.mp4': 'video_36', '4wU_LUjG5Ic.mp4': 'video_30', 'EYqVtI9YWJA.mp4': 'video_42', 'LRw_obCPUt0.mp4': 'video_20', 'XkqCExn6_Us.mp4': 'video_23', '91IHQYk1IQM.mp4': 'video_26', 'fWutDQy1nnY.mp4': 'video_29', 'NyBmCxDoHJU.mp4': 'video_47', 'xmEERLqJ2kU.mp4': 'video_33', '98MoyGZKHXc.mp4': 'video_2', 'GsAD1KT1xo8.mp4': 'video_24', 'oDXZc0tZe04.mp4': 'video_40', '_xMr-HKMfVA.mp4': 'video_35', 'akI8YFjEmUw.mp4': 'video_10', 'gzDbaEs1Rlg.mp4': 'video_4', 'PJrm840pAUI.mp4': 'video_25', 'xwqBXPGE9pQ.mp4': 'video_9', 'AwmHb44_ouw.mp4': 'video_1', 'Hl-__g2gn_A.mp4': 'video_17', 'qqR6AEXwxoQ.mp4': 'video_41', 'xxdtq8mxegs.mp4': 'video_15', 'b626MiF1ew4.mp4': 'video_22', 'HT5vyqe0Xaw.mp4': 'video_6', 'RBCABdttQmI.mp4': 'video_27', 'XzYM3PfTM4w.mp4': 'video_5', 'Bhxk-O1Y7Ho.mp4': 'video_12', 'i3wAGJaaktw.mp4': 'video_11', 'Yi4Ij2NM7U4.mp4': 'video_18', 'byxOvuiIJV0.mp4': 'video_34', 'iVt07TCkFM0.mp4': 'video_45', 'sTEELN-vY30.mp4': 'video_7', 'z_6gVvQb2d0.mp4': 'video_28', 'cjibtmSLxQ4.mp4': 'video_21', 'J0nA4VgnoCo.mp4': 'video_3', 'uGu_10sucQo.mp4': 'video_37', 'E11zDS9XGzg.mp4': 'video_46', 'jcoYJXDG9sw.mp4': 'video_49', 'vdmoEJ5YbrQ.mp4': 'video_8', 'WG0MBPpPC6I.mp4': 'video_16', 'Se3oxnaPsz0.mp4': 'video_39'}\n",
            "{'video_13': '0tmA_C6XwfM.mp4', 'video_38': 'EE-bNr36nyA.mp4', 'video_44': 'JgHubY5Vw3Y.mp4', 'video_31': 'VuWGsYPqAX8.mp4', 'video_19': '37rzWOQsNIw.mp4', 'video_43': 'eQu1rNs0an0.mp4', 'video_32': 'JKpqYvAdIsw.mp4', 'video_14': '3eYKfiOEJNs.mp4', 'video_50': '-esJrBWj2d8.mp4', 'video_48': 'kLxoNp-UchI.mp4', 'video_36': 'WxtbjNsCQ8A.mp4', 'video_30': '4wU_LUjG5Ic.mp4', 'video_42': 'EYqVtI9YWJA.mp4', 'video_20': 'LRw_obCPUt0.mp4', 'video_23': 'XkqCExn6_Us.mp4', 'video_26': '91IHQYk1IQM.mp4', 'video_29': 'fWutDQy1nnY.mp4', 'video_47': 'NyBmCxDoHJU.mp4', 'video_33': 'xmEERLqJ2kU.mp4', 'video_2': '98MoyGZKHXc.mp4', 'video_24': 'GsAD1KT1xo8.mp4', 'video_40': 'oDXZc0tZe04.mp4', 'video_35': '_xMr-HKMfVA.mp4', 'video_10': 'akI8YFjEmUw.mp4', 'video_4': 'gzDbaEs1Rlg.mp4', 'video_25': 'PJrm840pAUI.mp4', 'video_9': 'xwqBXPGE9pQ.mp4', 'video_1': 'AwmHb44_ouw.mp4', 'video_17': 'Hl-__g2gn_A.mp4', 'video_41': 'qqR6AEXwxoQ.mp4', 'video_15': 'xxdtq8mxegs.mp4', 'video_22': 'b626MiF1ew4.mp4', 'video_6': 'HT5vyqe0Xaw.mp4', 'video_27': 'RBCABdttQmI.mp4', 'video_5': 'XzYM3PfTM4w.mp4', 'video_12': 'Bhxk-O1Y7Ho.mp4', 'video_11': 'i3wAGJaaktw.mp4', 'video_18': 'Yi4Ij2NM7U4.mp4', 'video_34': 'byxOvuiIJV0.mp4', 'video_45': 'iVt07TCkFM0.mp4', 'video_7': 'sTEELN-vY30.mp4', 'video_28': 'z_6gVvQb2d0.mp4', 'video_21': 'cjibtmSLxQ4.mp4', 'video_3': 'J0nA4VgnoCo.mp4', 'video_37': 'uGu_10sucQo.mp4', 'video_46': 'E11zDS9XGzg.mp4', 'video_49': 'jcoYJXDG9sw.mp4', 'video_8': 'vdmoEJ5YbrQ.mp4', 'video_16': 'WG0MBPpPC6I.mp4', 'video_39': 'Se3oxnaPsz0.mp4'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9RdxiWiHf4g"
      },
      "source": [
        "### Frame extraction\n",
        "\n",
        "as state in keras doc:\n",
        "\n",
        "pooling: Optional pooling mode for feature extraction when include_top is False.\n",
        "\n",
        "* None (default) means that the output of the model will be the 4D tensor output of the last convolutional block.\n",
        "* avg means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor.\n",
        "* max means that global max pooling will be applied.\n",
        "\n",
        "Guide to create h5 file here:\n",
        "https://github.com/KaiyangZhou/vsumm-reinforce/issues/1#issuecomment-363492711\n",
        "\n",
        "``` python\n",
        "import h5py\n",
        "h5_file_name = 'blah blah blah'\n",
        "f = h5py.File(h5_file_name, 'w')\n",
        "\n",
        "# video_names is a list of strings containing the\n",
        "# name of a video, e.g. 'video_1', 'video_2'\n",
        "for name in video_names:\n",
        "    f.create_dataset(name + '/features', data=data_of_name)\n",
        "    f.create_dataset(name + '/gtscore', data=data_of_name)\n",
        "    f.create_dataset(name + '/user_summary', data=data_of_name)\n",
        "    f.create_dataset(name + '/change_points', data=data_of_name)\n",
        "    f.create_dataset(name + '/n_frame_per_seg', data=data_of_name)\n",
        "    f.create_dataset(name + '/n_frames', data=data_of_name)\n",
        "    f.create_dataset(name + '/picks', data=data_of_name)\n",
        "    f.create_dataset(name + '/n_steps', data=data_of_name)\n",
        "    f.create_dataset(name + '/gtsummary', data=data_of_name)\n",
        "    f.create_dataset(name + '/video_name', data=data_of_name)\n",
        "\n",
        "f.close()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el2ecnqwArmZ"
      },
      "source": [
        "file_name_to_h5 = {'0tmA_C6XwfM.mp4': 'video_13', 'EE-bNr36nyA.mp4': 'video_38', 'JgHubY5Vw3Y.mp4': 'video_44', 'VuWGsYPqAX8.mp4': 'video_31', '37rzWOQsNIw.mp4': 'video_19', 'eQu1rNs0an0.mp4': 'video_43', 'JKpqYvAdIsw.mp4': 'video_32', '3eYKfiOEJNs.mp4': 'video_14', '-esJrBWj2d8.mp4': 'video_50', 'kLxoNp-UchI.mp4': 'video_48', 'WxtbjNsCQ8A.mp4': 'video_36', '4wU_LUjG5Ic.mp4': 'video_30', 'EYqVtI9YWJA.mp4': 'video_42', 'LRw_obCPUt0.mp4': 'video_20', 'XkqCExn6_Us.mp4': 'video_23', '91IHQYk1IQM.mp4': 'video_26', 'fWutDQy1nnY.mp4': 'video_29', 'NyBmCxDoHJU.mp4': 'video_47', 'xmEERLqJ2kU.mp4': 'video_33', '98MoyGZKHXc.mp4': 'video_2', 'GsAD1KT1xo8.mp4': 'video_24', 'oDXZc0tZe04.mp4': 'video_40', '_xMr-HKMfVA.mp4': 'video_35', 'akI8YFjEmUw.mp4': 'video_10', 'gzDbaEs1Rlg.mp4': 'video_4', 'PJrm840pAUI.mp4': 'video_25', 'xwqBXPGE9pQ.mp4': 'video_9', 'AwmHb44_ouw.mp4': 'video_1', 'Hl-__g2gn_A.mp4': 'video_17', 'qqR6AEXwxoQ.mp4': 'video_41', 'xxdtq8mxegs.mp4': 'video_15', 'b626MiF1ew4.mp4': 'video_22', 'HT5vyqe0Xaw.mp4': 'video_6', 'RBCABdttQmI.mp4': 'video_27', 'XzYM3PfTM4w.mp4': 'video_5', 'Bhxk-O1Y7Ho.mp4': 'video_12', 'i3wAGJaaktw.mp4': 'video_11', 'Yi4Ij2NM7U4.mp4': 'video_18', 'byxOvuiIJV0.mp4': 'video_34', 'iVt07TCkFM0.mp4': 'video_45', 'sTEELN-vY30.mp4': 'video_7', 'z_6gVvQb2d0.mp4': 'video_28', 'cjibtmSLxQ4.mp4': 'video_21', 'J0nA4VgnoCo.mp4': 'video_3', 'uGu_10sucQo.mp4': 'video_37', 'E11zDS9XGzg.mp4': 'video_46', 'jcoYJXDG9sw.mp4': 'video_49', 'vdmoEJ5YbrQ.mp4': 'video_8', 'WG0MBPpPC6I.mp4': 'video_16', 'Se3oxnaPsz0.mp4': 'video_39'}\n",
        "h5_to_file_name = {'video_13': '0tmA_C6XwfM.mp4', 'video_38': 'EE-bNr36nyA.mp4', 'video_44': 'JgHubY5Vw3Y.mp4', 'video_31': 'VuWGsYPqAX8.mp4', 'video_19': '37rzWOQsNIw.mp4', 'video_43': 'eQu1rNs0an0.mp4', 'video_32': 'JKpqYvAdIsw.mp4', 'video_14': '3eYKfiOEJNs.mp4', 'video_50': '-esJrBWj2d8.mp4', 'video_48': 'kLxoNp-UchI.mp4', 'video_36': 'WxtbjNsCQ8A.mp4', 'video_30': '4wU_LUjG5Ic.mp4', 'video_42': 'EYqVtI9YWJA.mp4', 'video_20': 'LRw_obCPUt0.mp4', 'video_23': 'XkqCExn6_Us.mp4', 'video_26': '91IHQYk1IQM.mp4', 'video_29': 'fWutDQy1nnY.mp4', 'video_47': 'NyBmCxDoHJU.mp4', 'video_33': 'xmEERLqJ2kU.mp4', 'video_2': '98MoyGZKHXc.mp4', 'video_24': 'GsAD1KT1xo8.mp4', 'video_40': 'oDXZc0tZe04.mp4', 'video_35': '_xMr-HKMfVA.mp4', 'video_10': 'akI8YFjEmUw.mp4', 'video_4': 'gzDbaEs1Rlg.mp4', 'video_25': 'PJrm840pAUI.mp4', 'video_9': 'xwqBXPGE9pQ.mp4', 'video_1': 'AwmHb44_ouw.mp4', 'video_17': 'Hl-__g2gn_A.mp4', 'video_41': 'qqR6AEXwxoQ.mp4', 'video_15': 'xxdtq8mxegs.mp4', 'video_22': 'b626MiF1ew4.mp4', 'video_6': 'HT5vyqe0Xaw.mp4', 'video_27': 'RBCABdttQmI.mp4', 'video_5': 'XzYM3PfTM4w.mp4', 'video_12': 'Bhxk-O1Y7Ho.mp4', 'video_11': 'i3wAGJaaktw.mp4', 'video_18': 'Yi4Ij2NM7U4.mp4', 'video_34': 'byxOvuiIJV0.mp4', 'video_45': 'iVt07TCkFM0.mp4', 'video_7': 'sTEELN-vY30.mp4', 'video_28': 'z_6gVvQb2d0.mp4', 'video_21': 'cjibtmSLxQ4.mp4', 'video_3': 'J0nA4VgnoCo.mp4', 'video_37': 'uGu_10sucQo.mp4', 'video_46': 'E11zDS9XGzg.mp4', 'video_49': 'jcoYJXDG9sw.mp4', 'video_8': 'vdmoEJ5YbrQ.mp4', 'video_16': 'WG0MBPpPC6I.mp4', 'video_39': 'Se3oxnaPsz0.mp4'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfteF_PON6Ot"
      },
      "source": [
        "h5file = '/content/drive/MyDrive/VSum/Colab/VASNet/datasets/eccv16_dataset_tvsum_google_pool5.h5' #AN\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "data = h5py.File(h5file, 'r')\n",
        "\n",
        "video_dir = '/content/drive/MyDrive/VSum/Colab/ydata-tvsum50-v1_1/video' ##AN\n",
        "video_list = [\"0tmA_C6XwfM.mp4\", \"EE-bNr36nyA.mp4\", \"JgHubY5Vw3Y.mp4\", \"VuWGsYPqAX8.mp4\", \"37rzWOQsNIw.mp4\", \"eQu1rNs0an0.mp4\", \"JKpqYvAdIsw.mp4\", \"WG0MBPpPC6I.mp4\", \"3eYKfiOEJNs.mp4\", \"-esJrBWj2d8.mp4\", \"kLxoNp-UchI.mp4\", \"WxtbjNsCQ8A.mp4\", \"4wU_LUjG5Ic.mp4\", \"EYqVtI9YWJA.mp4\", \"LRw_obCPUt0.mp4\", \"XkqCExn6_Us.mp4\", \"91IHQYk1IQM.mp4\", \"fWutDQy1nnY.mp4\", \"NyBmCxDoHJU.mp4\", \"xmEERLqJ2kU.mp4\", \"98MoyGZKHXc.mp4\", \"GsAD1KT1xo8.mp4\", \"oDXZc0tZe04.mp4\", \"_xMr-HKMfVA.mp4\", \"akI8YFjEmUw.mp4\", \"gzDbaEs1Rlg.mp4\", \"PJrm840pAUI.mp4\", \"xwqBXPGE9pQ.mp4\", \"AwmHb44_ouw.mp4\", \"Hl-__g2gn_A.mp4\", \"qqR6AEXwxoQ.mp4\", \"xxdtq8mxegs.mp4\", \"b626MiF1ew4.mp4\", \"HT5vyqe0Xaw.mp4\", \"RBCABdttQmI.mp4\", \"XzYM3PfTM4w.mp4\", \"Bhxk-O1Y7Ho.mp4\", \"i3wAGJaaktw.mp4\", \"Se3oxnaPsz0.mp4\", \"Yi4Ij2NM7U4.mp4\", \"byxOvuiIJV0.mp4\", \"iVt07TCkFM0.mp4\", \"sTEELN-vY30.mp4\", \"z_6gVvQb2d0.mp4\", \"cjibtmSLxQ4.mp4\", \"J0nA4VgnoCo.mp4\", \"uGu_10sucQo.mp4\", \"E11zDS9XGzg.mp4\", \"jcoYJXDG9sw.mp4\", \"vdmoEJ5YbrQ.mp4\"]\n",
        "\n",
        "\n",
        "new_h5_file = '/content/drive/MyDrive/VSum/Colab/eccv16_dataset_tvsum_google_pool5-replace-by-inceptionv3.h5'\n",
        "f = h5py.File(new_h5_file, 'w')\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "import cv2\n",
        "import sys\n",
        "\n",
        "model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
        "target_size=(299, 299)\n",
        "\n",
        "d = dict()\n",
        "for name in data:\n",
        "  video_path = video_dir + \"/\" + h5_to_file_name[name]\n",
        "  vs = cv2.VideoCapture(video_path)\n",
        "  n_frames = 0\n",
        "\n",
        "\n",
        "  features = np.empty((1,2048))\n",
        "  pick = []\n",
        "\n",
        "  print(\"Processing \", name, h5_to_file_name[name])\n",
        "\n",
        "  total =  np.asarray(data[name]['n_frames']) + 0\n",
        "  while True:\n",
        "    # read the next frame from the file\n",
        "    (grabbed, frame) = vs.read()\n",
        "    # if the frame was not grabbed, then we have reached the end\n",
        "    # of the stream\n",
        "    if not grabbed:\n",
        "      break\n",
        "    n_frames += 1\n",
        "\n",
        "\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = cv2.resize(frame, target_size).astype(\"float32\")\n",
        "    x = image.img_to_array(frame)\n",
        "\n",
        "    percentage = n_frames * 100 / total\n",
        "    sys.stdout.write('\\r')\n",
        "    # the exact output you're looking for:\n",
        "    sys.stdout.write(\"[%-50s] %.2f%%\" % ('='*(int(percentage)//2), percentage))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    if (n_frames % 15 == 0): ## Sample 1 frame out of 15\n",
        "      x = np.expand_dims(x, axis=0)\n",
        "      x = preprocess_input(x)\n",
        "      feature = model.predict(x)\n",
        "      features = np.vstack( [features, feature])\n",
        "      pick.append(n_frames)\n",
        "\n",
        "  f.create_dataset(name + '/features', data=features)\n",
        "  f.create_dataset(name + '/gtscore', data=data[name]['gtscore'])\n",
        "  f.create_dataset(name + '/user_summary', data=data[name]['user_summary'])\n",
        "  f.create_dataset(name + '/change_points', data=data[name]['change_points'])\n",
        "  f.create_dataset(name + '/n_frame_per_seg', data=data[name]['n_frame_per_seg'])\n",
        "  f.create_dataset(name + '/n_frames', data=data[name]['n_frames'])\n",
        "  f.create_dataset(name + '/picks', data=data[name]['picks'])\n",
        "  f.create_dataset(name + '/n_steps', data=data[name]['n_steps'])\n",
        "  f.create_dataset(name + '/gtsummary', data=data[name]['gtsummary'])\n",
        "  f.create_dataset(name + '/video_name', data=h5_to_file_name[name])\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YohaFeL3H4Ya"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "vs = cv2.VideoCapture(test_video)\n",
        "writer = None\n",
        "(W, H) = (None, None)\n",
        "# loop over frames from the video file stream\n",
        "\n",
        "model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
        "target_size=(299, 299)\n",
        "\n",
        "count = 0\n",
        "features = np.empty((1,2048))\n",
        "pick = []\n",
        "# features.shape=(1,2048)\n",
        "while True:\n",
        "\t# read the next frame from the file\n",
        "  (grabbed, frame) = vs.read()\n",
        "\t# if the frame was not grabbed, then we have reached the end\n",
        "\t# of the stream\n",
        "  if not grabbed:\n",
        "    break\n",
        "\t# if the frame dimensions are empty, grab them\n",
        "  if W is None or H is None:\n",
        "    (H, W) = frame.shape[:2]\n",
        "  # clone the output frame, then convert it from BGR to RGB\n",
        "\t# ordering, resize the frame to a fixed 224x224, and then\n",
        "\t# perform mean subtraction\n",
        "  output = frame.copy()\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  frame = cv2.resize(frame, target_size).astype(\"float32\")\n",
        "  x = image.img_to_array(frame)\n",
        "\n",
        "  if (count == 500):\n",
        "    plt.imshow(x/255.)\n",
        "    break\n",
        "\n",
        "  if (count % 15 == 0): ## Sample 1 frame out of 15\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    feature = model.predict(x)\n",
        "    features = np.vstack( [features, feature])\n",
        "    pick.append(count)\n",
        "\n",
        "  #print(features.shape)\n",
        "  count = count + 1\n",
        "  #break\n",
        "\n",
        "\n",
        "print(features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}